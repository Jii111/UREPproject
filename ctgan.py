# -*- coding: utf-8 -*-
"""CTGAN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/117xCyi4ar1Lr3jlYGxfMPHTF8gf4qcTE
"""

# 필요한 라이브러리 로드
import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.layers import Conv2D, Activation, Dropout, Flatten, Dense, BatchNormalization, Reshape, UpSampling2D, Input
from keras.models import Model
from keras.optimizers import RMSprop

import random

# 시드 고정
def set_seed(seed=42):
    np.random.seed(seed)
    tf.random.set_seed(seed)
    random.seed(seed)

from google.colab import drive
drive.mount('/content/drive')

# 데이터 불러오기
finaltrain = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Urep/gan_data/final15kAll_train.csv')
finaltest = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Urep/gan_data/final15kAll_test.csv')

train = finaltrain
temp = train.loc[:,train.columns != 'y']
train.loc[:,train.columns != 'y'] = (temp - temp.mean()) / temp.std()

train_blca= train[train['y']=='blca']
train_normal= train[train['y']=='normal']
train_prad= train[train['y']=='prad']
train_kirc= train[train['y']=='kirc']

# 테스트 데이터 전처리
# y 열 분리
y_test = finaltest['y']
finaltest = finaltest.drop(columns=['y'])
finaltest = pd.get_dummies(finaltest)

# 테스트 데이터 수치형 데이터 정규화: 훈련 데이터의 스케일 사용
finaltest_numeric = (finaltest - finaltrain.mean()) / finaltrain.std()
y_test_encoded = encoder.transform(y_test.values.reshape(-1, 1))

# 테스트 데이터 정규화 후 원래 레이블과 결합
finaltest_df = pd.DataFrame(finaltest_numeric, columns=finaltest_numeric.columns)
finaltest_df['y'] = y_test  # y_test는 원래 테스트 데이터의 레이블

# CSV 파일로 저장
finaltest_df.to_csv('/content/drive/MyDrive/Colab Notebooks/Urep/gan_test_15k.csv', index=False)

train_prad.drop(columns=['y']).shape

from keras import layers

# Generator
generator = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(100,)),
    layers.BatchNormalization(),
    layers.Dense(256, activation='relu'),
    layers.BatchNormalization(),
    layers.Dense(512, activation='relu'),
    layers.BatchNormalization(),
    layers.Dense(768, activation='tanh')
])

# Discriminator
discriminator = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(768,)),
    layers.BatchNormalization(),
    layers.Dense(64, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])

from keras.optimizers import Adam

generator_optimizer = Adam(learning_rate=0.04, beta_1=0.5)
discriminator_optimizer = Adam(learning_rate=0.02, beta_1=0.5)


discriminator.compile(loss='binary_crossentropy', optimizer=discriminator_optimizer, metrics=['accuracy'])
discriminator.trainable = False  # 판별자 훈련 중지


model_input = Input(shape=(100, ))
model_output = discriminator(generator(model_input))
model = Model(model_input, model_output)

model.compile(loss='binary_crossentropy', optimizer=generator_optimizer, metrics=['accuracy'])

def train_discriminator(x_train, batch_size):
    valid = np.ones((batch_size, 1))
    fake = np.zeros((batch_size, 1))

    idx = np.random.randint(0, len(x_train), batch_size)
    true_imgs = x_train.iloc[idx]
    discriminator.fit(true_imgs, valid, verbose=0)

    noise = np.random.normal(0, 1, (batch_size, 100))
    gen_imgs = generator.predict(noise)

    discriminator.fit(gen_imgs, fake, verbose=0)

def train_generator(batch_size):
    valid = np.ones((batch_size, 1))
    noise = np.random.normal(0, 1, (batch_size, 100))
    model.fit(noise, valid, verbose=1)

for epoch in tqdm.tqdm(range(2000)):
    train_discriminator(train_normal.drop(columns=['y']), 64)
    train_generator(64)

import tqdm
for epoch in tqdm.tqdm(range(2000)):
    train_discriminator(train_normal.drop(columns=['y']), 64)
    train_generator(64)

generator.summary()

from keras.optimizers import Adam

generator_optimizer = Adam(learning_rate=0.04, beta_1=0.5)
discriminator_optimizer = Adam(learning_rate=0.02, beta_1=0.5)


discriminator.compile(loss='binary_crossentropy', optimizer=discriminator_optimizer, metrics=['accuracy'])
discriminator.trainable = False  # 판별자 훈련 중지


model_input = Input(shape=(100, ))
model_output = discriminator(generator(model_input))
model = Model(model_input, model_output)

model.compile(loss='binary_crossentropy', optimizer=generator_optimizer, metrics=['accuracy'])

import tqdm

def train_discriminator(x_train, batch_size):
    valid = np.ones((batch_size, 1))
    fake = np.zeros((batch_size, 1))

    idx = np.random.randint(0, len(x_train), batch_size)
    true_imgs = x_train.iloc[idx]
    discriminator.fit(true_imgs, valid, verbose=0)

    noise = np.random.normal(0, 1, (batch_size, 100))
    gen_imgs = generator.predict(noise)

    discriminator.fit(gen_imgs, fake, verbose=0)

def train_generator(batch_size):
    valid = np.ones((batch_size, 1))
    noise = np.random.normal(0, 1, (batch_size, 100))
    model.fit(noise, valid, verbose=1)

def train_discriminator(x_train, batch_size):
    valid = np.ones((batch_size, 1))
    fake = np.zeros((batch_size, 1))

    idx = np.random.randint(0, len(x_train), batch_size)
    true_imgs = x_train.iloc[idx]

    # true_imgs 형태 출력
    print("true_imgs shape:", true_imgs.shape)

    discriminator.fit(true_imgs, valid, verbose=0)

    noise = np.random.normal(0, 1, (batch_size, 100))
    gen_imgs = generator.predict(noise)

    discriminator.fit(gen_imgs, fake, verbose=0)

for epoch in tqdm.tqdm(range(2000)):
    train_discriminator(train_normal.drop(columns=['y']), 64)
    train_generator(64)

train_normal.drop(columns=['y']).shape

import tqdm
for epoch in tqdm.tqdm(range(2000)):
    train_discriminator(train_normal.drop(columns=['y']), 64)
    train_generator(64)







set_seed(42)

# 배치 크기를 데이터의 행 개수로 설정
batch_size = len(finaltrain)

# 훈련 과정
for epoch in range(1000):
    # 진짜 데이터 샘플링
    real_data_indices = np.random.randint(0, finaltrain_numeric.shape[0], batch_size)
    real_data = finaltrain_numeric.iloc[real_data_indices].values

    # 가짜 데이터 생성
    noise = np.random.normal(0, 1, (batch_size, 100))
    fake_data = generator.predict(noise)

    # 판별자 훈련
    discriminator_loss_real = discriminator.train_on_batch(real_data, np.ones((batch_size, 1)))
    discriminator_loss_fake = discriminator.train_on_batch(fake_data, np.zeros((batch_size, 1)))

    # 생성자 훈련
    noise = np.random.normal(0, 1, (batch_size, 100))
    generator_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))

    # 훈련 진행 상황 출력
    if epoch % 100 == 0:
        print(f"Epoch: {epoch}, Discriminator Loss Real: {discriminator_loss_real[0]}, "
              f"Discriminator Loss Fake: {discriminator_loss_fake[0]}, "
              f"Generator Loss: {generator_loss}")

set_seed(42)

# 배치 크기 설정 - 데이터 불리는 배수 설정하는 버전
batch_size = len(finaltrain)  # 실제 데이터 크기
multiplier = 5  # 데이터 5배로 불리기

# 훈련 과정
for epoch in range(1000):

    # 진짜 데이터 샘플링
    real_data_indices = np.random.randint(0, finaltrain_numeric.shape[0], batch_size)
    real_data = finaltrain_numeric.iloc[real_data_indices].values

    # 가짜 데이터 생성 (5배로 생성)
    noise = np.random.normal(0, 1, (batch_size * multiplier, 100))
    fake_data = generator.predict(noise)

    # 판별자 훈련
    discriminator_loss_real = discriminator.train_on_batch(real_data, np.ones((batch_size, 1)))
    discriminator_loss_fake = discriminator.train_on_batch(fake_data, np.zeros((batch_size * multiplier, 1)))

    # 생성자 훈련
    noise = np.random.normal(0, 1, (batch_size * multiplier, 100))
    generator_loss = gan.train_on_batch(noise, np.ones((batch_size * multiplier, 1)))

    # 훈련 진행 상황 출력 (100 epoch마다)
    if epoch % 100 == 0:
        print(f"Epoch: {epoch}, Discriminator Loss Real: {discriminator_loss_real[0]}, "
              f"Discriminator Loss Fake: {discriminator_loss_fake[0]}, "
              f"Generator Loss: {generator_loss}")

# GAN으로 생성된 가짜 데이터를 기존 train 데이터와 결합
augmented_train_data = np.vstack((finaltrain_numeric, fake_data))  # 데이터 결합

# 원본 데이터에서 레이블 문자열을 가져옴
class_labels = encoder.categories_[0]  # 원-핫 인코딩된 클래스 레이블 가져오기

# 가짜 데이터의 레이블을 원본 데이터의 문자열 레이블 기준으로 무작위로 샘플링
fake_labels = np.random.choice(class_labels, size=fake_data.shape[0], replace=True)

# 기존 레이블과 가짜 데이터 레이블 결합
augmented_train_labels = np.hstack((y_train, fake_labels))  # 기존 레이블과 가짜 데이터 레이블 결합

# DataFrame으로 변환 (결합된 데이터와 레이블)
augmented_train_df = pd.DataFrame(augmented_train_data, columns=finaltrain_numeric.columns)
augmented_train_df['y'] = augmented_train_labels  # 레이블 컬럼 추가

# CSV 파일로 저장
augmented_train_df.to_csv('/content/drive/MyDrive/Colab Notebooks/Urep/gan_data/gan6_train_15k.csv', index=False)

! pip install tensorflow-gan

import tqdm
import tensorflow_gan as tfgan
import tensorflow as tf
import numpy as np

def train_discriminator(x_train, batch_size):
    valid = np.ones((batch_size, 1))
    fake = np.zeros((batch_size, 1))

    idx = np.random.randint(0, len(x_train), batch_size)
    true_imgs = x_train.iloc[idx]
    discriminator.fit(true_imgs, valid, verbose=0)

    noise = np.random.normal(0, 1, (batch_size, 100))
    gen_imgs = generator.predict(noise)

    discriminator.fit(gen_imgs, fake, verbose=0)

def train_generator(batch_size):
    valid = np.ones((batch_size, 1))
    noise = np.random.normal(0, 1, (batch_size, 100))
    model.fit(noise, valid, verbose=1)

gan_estimator = tfgan.estimator.GANEstimator(
    generator_fn=train_generator,
    discriminator_fn=train_discriminator,
    generator_loss_fn=tfgan.losses.minimax_generator_loss,
    discriminator_loss_fn=tfgan.losses.minimax_discriminator_loss,
    generator_optimizer=tf.optimizers.Adam(0.001),
    discriminator_optimizer=tf.optimizers.Adam(0.001)
)

data_dim = 760  # 데이터의 차원
batch_size = 64
num_steps = 2000

# 학습 데이터 입력 함수 정의
def input_fn():
    real_data = train_normal.drop(columns=['y'])
    noise = np.random.normal(size=(batch_size, 100)).astype(np.float32)  # 노이즈 입력
    labels = np.ones((batch_size, 1), dtype=np.float32)  # GANEstimator에서 진짜/가짜 라벨로 사용
    dataset = tf.data.Dataset.from_tensor_slices(((real_data, noise), labels))
    dataset = dataset.shuffle(buffer_size=1000).repeat().batch(batch_size)
    return dataset

# GAN 학습 실행
gan_estimator.train(input_fn, steps=num_steps)

import tensorflow as tf
import tensorflow_gan as tfgan
import numpy as np

# 생성자 모델 정의
def generator_fn(noise):
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(128, activation='relu', input_shape=(100,)))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Dense(256, activation='relu'))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Dense(512, activation='relu'))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Dense(760, activation='tanh'))  # 데이터 차원에 맞추기
    return model(noise)

# 판별자 모델 정의
def discriminator_fn(data):
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(512, activation='relu', input_shape=(760,)))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Dense(256, activation='relu'))
    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 이진 분류
    return model(data)

# GANEstimator 정의
gan_estimator = tfgan.estimator.GANEstimator(
    generator_fn=generator_fn,
    discriminator_fn=discriminator_fn,
    generator_loss_fn=tfgan.losses.minimax_generator_loss,
    discriminator_loss_fn=tfgan.losses.minimax_discriminator_loss,
    generator_optimizer=tf.optimizers.Adam(0.001),
    discriminator_optimizer=tf.optimizers.Adam(0.001)
)

# 학습 데이터 준비
train_data = train_normal.drop(columns=['y']).values.astype(np.float32)

# 입력 함수 정의
def input_fn():
    real_data = train_data
    noise = np.random.normal(size=(batch_size, 100)).astype(np.float32)  # 노이즈 입력
    labels = np.ones((batch_size, 1), dtype=np.float32)  # 진짜 데이터에 대한 라벨
    dataset = tf.data.Dataset.from_tensor_slices((real_data, noise)).shuffle(buffer_size=1000).batch(batch_size)
    return dataset

# GAN 학습 실행
batch_size = 64
num_steps = 2000
gan_estimator.train(input_fn, steps=num_steps)

import tensorflow as tf
print(tf.__version__)

!pip install tensorflow-estimator

#pip uninstall tensorflow tensorboard tensorflow-estimator
#!pip uninstall tensorflow
!pip install tensorflow

! pip install ctgan

# 데이터 불러오기
import pandas as pd
finaltrain = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Urep/gan_data/final15kAll_train.csv')
finaltest = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Urep/gan_data/final15kAll_test.csv')

finaltrain['y'].value_counts()
# train_normal.drop(columns=['y'])

finaltrain

real_data

from ctgan import CTGAN
from ctgan import load_demo

real_data = load_demo()

# Names of the columns that are discrete
discrete_columns = ['y']

ctgan = CTGAN(epochs=10)
ctgan.fit(finaltrain, discrete_columns)

# Create synthetic data
synthetic_data = ctgan.sample(760)

synthetic_data

95*8

synthetic_data.to_csv("/content/drive/MyDrive/Colab Notebooks/Urep/gan_data/CTgan760.csv",index=False)

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

import random

# 시드 고정
def set_seed(seed=42):
    np.random.seed(seed)
    random.seed(seed)

# 데이터 불러오기
finaltrain = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Urep/gan_data/cgan2.csv')
#finaltrain = finaltrain.drop(columns=['Unnamed: 0'])
finaltest = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Urep/gan_data/gan_test_15k.csv')

X_train = finaltrain.drop(columns=['y'])
y_train = finaltrain['y']
X_test = finaltest.drop(columns=['y'])
y_test = finaltest['y']

finaltrain

# 모델 생성 및 학습 - decision Tree
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)

# 예측 및 성능 평가
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
auc_score = roc_auc_score(y_test, model.predict_proba(X_test), multi_class='ovr')

print("Accuracy:", accuracy)
print("AUC Score:", auc_score)

# 모델 생성 및 학습 - random forest
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

# 예측 및 성능 평가
y_pred = rf_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
auc_score = roc_auc_score(y_test, rf_model.predict_proba(X_test), multi_class='ovr')

print("Accuracy:", accuracy)
print("AUC Score:", auc_score)

train_blca= train[train['y']=='blca']
train_normal= train[train['y']=='normal']
train_prad= train[train['y']=='prad']
train_kirc= train[train['y']=='kirc']